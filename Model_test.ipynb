{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import_preprocessing import import_data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    gpu_num = 1\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[gpu_num], 'GPU')\n",
    "        #tf.config.experimental.set_memory_growth(gpus[gpu_num], True)\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[gpu_num],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_name = 'data/nb_data_changed_HT_1-1.csv'\n",
    "test_data_name = 'data/nb_data_changed_HT_1-2.csv'\n",
    "preprocessing = import_data_preprocessing(train_data_file_name = train_data_name,\n",
    "                                          test_data_file_name= test_data_name,\n",
    "                                          \n",
    "                                         )\n",
    "print('train_data : ', preprocessing.train_data_file_sample_column)\n",
    "print('train_data : ', preprocessing.test_data_file_sample_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing(sgRNA_column='target',\n",
    "                    indel_column='indel_percent',\n",
    "                    split_data=0.1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.keys())\n",
    "print(data['train'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['train']['seq']\n",
    "class_train = data['train']['indel_class']\n",
    "rate_train = data['train']['indel_rate']\n",
    "\n",
    "X_val = data['val']['seq']\n",
    "class_val = data['val']['indel_class']\n",
    "rate_val = data['val']['indel_rate']\n",
    "\n",
    "X_test = data['test']['seq']\n",
    "class_test = data['test']['indel_class']\n",
    "rate_test = data['test']['indel_rate']\n",
    "\n",
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "en_lr = 0.001\n",
    "en_epochs = 500\n",
    "\n",
    "dnn_lr = 0.001\n",
    "dnn_epochs = 400\n",
    "\n",
    "\n",
    "dropout_RNN = 0.5\n",
    "dropout_RNN_out = 0.5\n",
    "dropout_DNN = 0.5\n",
    "dropout2_DNN = 0.5\n",
    "\n",
    "\n",
    "DNN_1 = 100\n",
    "DNN_2 = 100\n",
    "DNN_3 = 100\n",
    "\n",
    "DNN_num_mis_1 = 100\n",
    "DNN_num_mis_2 = 100\n",
    "DNN_num_mis_3 = 5\n",
    "\n",
    "DNN_class_1 = 100\n",
    "DNN_class_2 = 100\n",
    "DNN_class_3 = 11\n",
    "\n",
    "DNN_rate_1 = 100\n",
    "DNN_rate_2 = 100\n",
    "DNN_rate_3 = 1\n",
    "\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNLSTM_input = Input(shape=input_shape)\n",
    "#CNNLSTM_input_reshape = Reshape((23,8))(CNNLSTM_input)\n",
    "\n",
    "kernal0 = 16\n",
    "dropout0 = 0.5\n",
    "CNN0_1 = Conv1D(kernal0, [1], activation='relu', padding='same', kernel_initializer='he_normal')(CNNLSTM_input)\n",
    "CNN0_1 = Dropout(0.3)(CNN0_1)\n",
    "\n",
    "CNN0_2 = Conv1D(kernal0, [2], activation='relu', padding='same', kernel_initializer='he_normal')(CNNLSTM_input)\n",
    "CNN0_2 = Dropout(0.3)(CNN0_2)\n",
    "\n",
    "CNN0_3 = Conv1D(kernal0, [3], activation='relu', padding='same', kernel_initializer='he_normal')(CNNLSTM_input)\n",
    "CNN0_3 = Dropout(0.3)(CNN0_3)\n",
    "\n",
    "CNN0_4 = Conv1D(kernal0, [4], activation='relu', padding='same', kernel_initializer='he_normal')(CNNLSTM_input)\n",
    "CNN0_4 = Dropout(0.3)(CNN0_4)\n",
    "\n",
    "CNN0_5 = Conv1D(kernal0, [5], activation='relu', padding='same', kernel_initializer='he_normal')(CNNLSTM_input)\n",
    "CNN0_5 = Dropout(0.3)(CNN0_5)\n",
    "\n",
    "concatenate0 = concatenate([CNN0_1, CNN0_2, CNN0_3, CNN0_4, CNN0_5])\n",
    "\n",
    "#squeezed = Lambda(lambda x: keras.backend.squeeze(x,axis=2))(concatenate0)\n",
    "\n",
    "LSTM0 = LSTM(64, return_sequences=True)(concatenate0)\n",
    "LSTM0 = Dropout(dropout0)(LSTM0)\n",
    "\n",
    "concatenate1 = concatenate([LSTM0, CNNLSTM_input])\n",
    "\n",
    "\n",
    "kernal1 = 32\n",
    "dropout1 = 0.5\n",
    "CNN1_1 = Conv1D(kernal1, [1], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate1)\n",
    "CNN1_1 = Dropout(dropout1)(CNN1_1)\n",
    "\n",
    "CNN1_2 = Conv1D(kernal1, [2], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate1)\n",
    "CNN1_2 = Dropout(dropout1)(CNN1_2)\n",
    "\n",
    "CNN1_3 = Conv1D(kernal1, [3], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate1)\n",
    "CNN1_3 = Dropout(dropout1)(CNN1_3)\n",
    "\n",
    "CNN1_4 = Conv1D(kernal1, [4], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate1)\n",
    "CNN1_4 = Dropout(dropout1)(CNN1_4)\n",
    "\n",
    "CNN1_5 = Conv1D(kernal1, [5], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate1)\n",
    "CNN1_5 = Dropout(dropout1)(CNN1_5)\n",
    "\n",
    "CNN1_concatenate = concatenate([CNN1_1, CNN1_2, CNN1_3, CNN1_4, CNN1_5])\n",
    "\n",
    "LSTM1 = LSTM(128, return_sequences=True)(CNN1_concatenate)\n",
    "LSTM1 = Dropout(dropout1)(LSTM1)\n",
    "\n",
    "\n",
    "#concatenate2 = concatenate([CNNLSTM_input_reshape,LSTM1])\n",
    "concatenate2 = concatenate([LSTM1, LSTM0])\n",
    "kernal2 = 64\n",
    "dropout2 = 0.5\n",
    "CNN2_1 = Conv1D(kernal2, [1], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate2)\n",
    "#CNN2_1 = MaxPool1D()(CNN2_1)\n",
    "CNN2_1 = Dropout(dropout2)(CNN2_1)\n",
    "\n",
    "CNN2_2 = Conv1D(kernal2, [2], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate2)\n",
    "#CNN2_2 = MaxPool1D()(CNN2_2)\n",
    "CNN2_2 = Dropout(dropout2)(CNN2_2)\n",
    "\n",
    "CNN2_3 = Conv1D(kernal2, [3], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate2)\n",
    "#CNN2_3 = MaxPool1D()(CNN2_3)\n",
    "CNN2_3 = Dropout(dropout2)(CNN2_3)\n",
    "\n",
    "CNN2_4 = Conv1D(kernal2, [4], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate2)\n",
    "#CNN2_4 = MaxPool1D()(CNN2_4)\n",
    "CNN2_4 = Dropout(dropout2)(CNN2_4)\n",
    "\n",
    "CNN2_5 = Conv1D(kernal2, [5], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate2)\n",
    "#CNN2_5 = MaxPool1D()(CNN2_5)\n",
    "CNN2_5 = Dropout(dropout2)(CNN2_5)\n",
    "\n",
    "CNN2_concatenate = concatenate([CNN2_1, CNN2_2, CNN2_3, CNN2_4, CNN2_5])\n",
    "\n",
    "LSTM2 = LSTM(256, return_sequences=True)(CNN2_concatenate)\n",
    "LSTM2 = Dropout(dropout2)(LSTM2)\n",
    "\n",
    "\n",
    "#concatenate3 = concatenate([CNNLSTM_input_reshape,LSTM2])\n",
    "concatenate3 = concatenate([LSTM2,LSTM1])\n",
    "kernal3 = 128\n",
    "dropout3 = 0.5\n",
    "CNN3_1 = Conv1D(kernal3, [1], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate3)\n",
    "#CNN3_1 = MaxPool1D()(CNN3_1)\n",
    "CNN3_1 = Dropout(dropout3)(CNN3_1)\n",
    "\n",
    "CNN3_2 = Conv1D(kernal3, [2], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate3)\n",
    "#CNN3_2 = MaxPool1D()(CNN3_2)\n",
    "CNN3_2 = Dropout(dropout3)(CNN3_2)\n",
    "\n",
    "CNN3_3 = Conv1D(kernal3, [3], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate3)\n",
    "#CNN3_3 = MaxPool1D()(CNN3_3)\n",
    "CNN3_3 = Dropout(dropout3)(CNN3_3)\n",
    "\n",
    "CNN3_4 = Conv1D(kernal3, [4], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate3)\n",
    "#CNN3_4 = MaxPool1D()(CNN3_4)\n",
    "CNN3_4 = Dropout(dropout3)(CNN3_4)\n",
    "\n",
    "CNN3_5 = Conv1D(kernal3, [5], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate3)\n",
    "#CNN3_5 = MaxPool1D()(CNN3_5)\n",
    "CNN3_5 = Dropout(dropout3)(CNN3_5)\n",
    "\n",
    "CNN3_concatenate = concatenate([CNN3_1, CNN3_2, CNN3_3, CNN3_4, CNN3_5])\n",
    "\n",
    "LSTM3 = LSTM(256, return_sequences=True)(CNN3_concatenate)\n",
    "LSTM3 = Dropout(dropout3)(LSTM3)\n",
    "\n",
    "\n",
    "\n",
    "#concatenate4 = LSTM3#concatenate([CNNLSTM_input_reshape,LSTM2])\n",
    "concatenate4 =concatenate([LSTM3, LSTM2])\n",
    "\n",
    "kernal4 = 128\n",
    "dropout3 = 0.5\n",
    "CNN4_1 = Conv1D(kernal4, [1], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate4)\n",
    "#CNN4_1 = MaxPool1D()(CNN4_1)\n",
    "CNN4_1 = Dropout(dropout3)(CNN4_1)\n",
    "\n",
    "CNN4_2 = Conv1D(kernal4, [2], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate4)\n",
    "#CNN4_2 = MaxPool1D()(CNN4_2)\n",
    "CNN4_2 = Dropout(dropout3)(CNN4_2)\n",
    "\n",
    "CNN4_3 = Conv1D(kernal4, [3], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate4)\n",
    "#CNN4_3 = MaxPool1D()(CNN4_3)\n",
    "CNN4_3 = Dropout(dropout3)(CNN4_3)\n",
    "\n",
    "CNN4_4 = Conv1D(kernal4, [4], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate4)\n",
    "#CNN4_4 = MaxPool1D()(CNN4_4)\n",
    "CNN4_4 = Dropout(dropout3)(CNN4_4)\n",
    "\n",
    "CNN4_5 = Conv1D(kernal4, [5], activation='relu', padding='same', kernel_initializer='he_normal')(concatenate4)\n",
    "#CNN4_5 = MaxPool1D()(CNN4_5)\n",
    "CNN4_5 = Dropout(dropout3)(CNN4_5)\n",
    "\n",
    "CNN4_concatenate = concatenate([CNN4_1, CNN4_2, CNN4_3, CNN4_4, CNN4_5])\n",
    "\n",
    "LSTM4 = LSTM(256, return_sequences=True)(CNN4_concatenate)\n",
    "LSTM4 = Dropout(dropout3)(LSTM4)\n",
    "\n",
    "CNN_model = Model(CNNLSTM_input, LSTM4)\n",
    "#CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.keras.utils.plot_model(CNN_model, show_shapes=True)\n",
    "#graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_middel = Lambda(lambda x: x[:,-1,:])(LSTM2) #Flatten()(CNN2_concatenate)\n",
    "LSTM_middel_2 = Lambda(lambda x: x[:,-1,:])(LSTM3) #Flatten()(CNN2_concatenate)\n",
    "LSTM_result = Lambda(lambda x: x[:,-1,:])(LSTM4) #Flatten()(CNN3_concatenate)\n",
    "\"\"\"\n",
    "#DNN model - number of mismatch prediction\n",
    "DNN_nummis_layer_input = LSTM_middel_2\n",
    "DNN_nummis_layer = Dense(DNN_num_mis_1, activation='relu')(DNN_nummis_layer_input)\n",
    "DNN_nummis_layer = Dropout(dropout2_DNN)(DNN_nummis_layer)\n",
    "DNN_nummis_layer = Dense(DNN_num_mis_3, activation='softmax', name='num_mis')(DNN_nummis_layer)\n",
    "\n",
    "\"\"\"\n",
    "#DNN model - class prediction\n",
    "DNN_class_layer_input = LSTM_middel\n",
    "DNN_class_layer_mid = Dense(DNN_class_1, activation='relu')(DNN_class_layer_input)\n",
    "DNN_class_layer = Dropout(dropout2_DNN)(DNN_class_layer_mid)\n",
    "DNN_class_layer = Dense(DNN_class_2, activation='relu')(DNN_class_layer)\n",
    "DNN_class_layer = Dropout(dropout2_DNN)(DNN_class_layer)\n",
    "DNN_class_layer = Dense(DNN_class_3, activation='softmax', name='class_1')(DNN_class_layer)\n",
    "\n",
    "DNN_class_layer_input_2 = LSTM_middel_2\n",
    "DNN_class_layer_mid_2 = Dense(DNN_class_1, activation='relu')(DNN_class_layer_input_2)\n",
    "DNN_class_layer_2 = Dropout(dropout2_DNN)(DNN_class_layer_mid_2)\n",
    "DNN_class_layer_2 = Dense(DNN_class_2, activation='relu')(DNN_class_layer_2)\n",
    "DNN_class_layer_2 = Dropout(dropout2_DNN)(DNN_class_layer_2)\n",
    "DNN_class_layer_2 = Dense(DNN_class_3, activation='softmax', name='class_2')(DNN_class_layer_2)\n",
    "\n",
    "\n",
    "#DNN model - class prediction 2\n",
    "DNN_class_layer_input_result = LSTM_result\n",
    "DNN_class_layer_result = Dense(DNN_class_1, activation='relu')(DNN_class_layer_input_result)\n",
    "DNN_class_layer_result = Dropout(dropout2_DNN)(DNN_class_layer_result)\n",
    "DNN_class_layer_mid_result = Dense(DNN_class_2, activation='relu')(DNN_class_layer_result)\n",
    "DNN_class_layer_result = Dropout(dropout2_DNN)(DNN_class_layer_mid_result)\n",
    "DNN_class_layer_result = Dense(DNN_class_3, activation='softmax', name='class_result')(DNN_class_layer_result)\n",
    "\n",
    "\n",
    "#DNN model - real value prediction\n",
    "#DNN_rate_layer_input = LSTM_result\n",
    "#DNN_rate_layer = Dense(DNN_rate_1, activation='relu', kernel_initializer='he_normal')(DNN_rate_layer_input)\n",
    "#DNN_rate_layer = Dropout(dropout2_DNN)(DNN_rate_layer)\n",
    "DNN_rate_layer = Dense(DNN_rate_2, activation='relu', kernel_initializer='he_normal')(DNN_class_layer_mid_result)\n",
    "DNN_rate_layer = Dropout(dropout2_DNN)(DNN_rate_layer)\n",
    "DNN_rate_layer = Dense(DNN_rate_3, activation='linear', kernel_initializer='he_normal', name='rate')(DNN_rate_layer)\n",
    "\n",
    "\n",
    "\n",
    "MTL_model = Model(CNNLSTM_input, [DNN_class_layer, DNN_class_layer_2,\n",
    "                                  DNN_class_layer_result, DNN_rate_layer])\n",
    "\n",
    "\n",
    "cluster_model = Model(CNNLSTM_input,DNN_class_layer)\n",
    "regression_model = Model(CNNLSTM_input, DNN_rate_layer)\n",
    "cluster_tsne_model = Model(CNNLSTM_input, DNN_class_layer_mid_result)\n",
    "\n",
    "\n",
    "#MTL_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = Adam(lr=0.0001)\n",
    "#optimizer_2 = Adam(lr=0.0001)\n",
    "\n",
    "MTL_model.compile(optimizer='adam',\n",
    "              loss={\n",
    "                  #'num_mis': 'categorical_crossentropy',\n",
    "                  'class_1': 'categorical_crossentropy',\n",
    "                  'class_2': 'categorical_crossentropy',\n",
    "                  'class_result': 'categorical_crossentropy',\n",
    "                  'rate': 'mean_squared_error'},\n",
    "              loss_weights={\n",
    "                  #'num_mis': 0.5,\n",
    "                  'class_1': 1,\n",
    "                  'class_2': 1,\n",
    "                  'class_result': 0.5,\n",
    "                  'rate': 1},\n",
    "              metrics={\n",
    "                  #'num_mis': 'accuracy',\n",
    "                  'class_1': \"accuracy\",\n",
    "                  'class_2': \"accuracy\",\n",
    "                  'class_result': \"accuracy\"}\n",
    "             )\n",
    "'''\n",
    "regression_model.compile(optimizer='adam',\n",
    "                        loss={'rate' : 'mean_squared_error'},\n",
    "                        )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_log.txt\n",
    "from sklearn.utils import class_weight\n",
    "class_train_num = class_train.argmax(axis=-1)\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  np.unique(class_train_num),\n",
    "                                                  class_train_num\n",
    "                                                 )\n",
    "class_weights_dict = dict(enumerate(class_weights))#{ i : class_weights[i] for i in range(11)}\n",
    "print(\"class_weight\")\n",
    "print(class_weights_dict)\n",
    "\n",
    "class_wieghts_dict_tuned = class_weights_dict\n",
    "for i in range(0,11):\n",
    "    multiple_constant = 2\n",
    "    cutoff_class = 5\n",
    "    \n",
    "    if i <=cutoff_class:\n",
    "        class_wieghts_dict_tuned[i] = class_weights_dict[i]#1.0\n",
    "    else:\n",
    "        class_wieghts_dict_tuned[i] = class_weights_dict[i]#multiple_constant*np.tanh((class_weights_dict[i]-2)/2) + multiple_constant\n",
    "    \n",
    "print(\"sample_weight\")\n",
    "print(class_wieghts_dict_tuned)\n",
    "\n",
    "sample_weight = np.array([class_wieghts_dict_tuned[i] for i in class_train_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  min_delta=0.0005,\n",
    "                                                  patience=50, verbose=0, mode='auto')\n",
    "\n",
    "MTL_model.fit(x=X_train,\n",
    "              y={\n",
    "                  #'num_mis': num_mis_train,\n",
    "                  'class_1': class_train,\n",
    "                  'class_2': class_train,\n",
    "                  'class_result': class_train,\n",
    "                  'rate': rate_train},\n",
    "              validation_data=(X_val, {#'num_mis': num_mis_val,\n",
    "                                       'class_1': class_val,\n",
    "                                       'class_2': class_val,\n",
    "                                       'class_result': class_val,\n",
    "                                       'rate': rate_val}),\n",
    "              class_weight={\n",
    "                  'class_1' : class_weights_dict,\n",
    "                  'class_2' : class_weights_dict,\n",
    "                  'class_result' : class_weights_dict},\n",
    "              sample_weight={'rate' : sample_weight},\n",
    "              shuffle=True,\n",
    "              epochs=200,\n",
    "              batch_size=batch_size,\n",
    "              verbose=verbose,\n",
    "              #callbacks=[early_stopping]\n",
    "             )\n",
    "\"\"\"\n",
    "regression_model.fit(x=X_train,\n",
    "                     y=rate_train,\n",
    "                     validation_data=(X_val, rate_val),\n",
    "                     sample_weight={'rate' : sample_weight},\n",
    "                     shuffle=True,\n",
    "                     epochs = 300,\n",
    "                     batch_size=batch_size,\n",
    "                     callbacks=[early_stopping]\n",
    "                    \n",
    "                    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cpf1의 경우 55번 정도?\n",
    " - 50번대에서 끝날 경우 correlation은 좋지만 분포 형태가 좋지 못하다 위아래가 잘린다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = regression_model.predict(X_test)\n",
    "test_prediction = np.array(test_prediction).reshape(-1,)\n",
    "test_true = rate_test.reshape(-1,)\n",
    "\n",
    "test_correlation = np.corrcoef(test_prediction, test_true).flatten()[1]\n",
    "pearson_corr = pearsonr(test_prediction, test_true)\n",
    "spearman_corr = spearmanr(test_prediction, test_true)\n",
    "print('Correlation : {}'.format(test_correlation))\n",
    "print('Pearson Correlation : {}'.format(pearson_corr[0]))\n",
    "print('Spearman Correlation : {}'.format(spearman_corr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_result_acc = MTL_model.evaluate(X_test,\n",
    "                                      {\n",
    "                                          #'num_mis': num_mis_test,\n",
    "                                          'class_1': class_test,\n",
    "                                          'class_2': class_test,\n",
    "                                          'class_result': class_test,\n",
    "                                          'rate': rate_test},\n",
    "                                      verbose=0\n",
    "                                     )\n",
    "#print(MTL_model.metrics_names)\n",
    "#print(class_result_acc)\n",
    "result_label = MTL_model.metrics_names\n",
    "for label, result in zip(result_label, class_result_acc):\n",
    "    print('{:16} : {:>6.4f}'.format(label,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indel_predict = test_prediction\n",
    "indel_true = test_true\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(x=indel_true, y=indel_predict)\n",
    "ax.set_title('Test data', fontsize=20)\n",
    "ax.set_xlabel('True', fontsize=15)\n",
    "ax.set_ylabel('Prediction', fontsize=15)\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,1)\n",
    "ax.grid(True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = regression_model.predict(X_train).reshape(-1,)\n",
    "train_true = rate_train.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indel_predict = train_prediction\n",
    "indel_true = train_true\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(x=indel_true, y=indel_predict)\n",
    "ax.set_title('Train data', fontsize=20)\n",
    "ax.set_xlabel('True', fontsize=15)\n",
    "ax.set_ylabel('Prediction', fontsize=15)\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,1)\n",
    "ax.grid(True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
